<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Eval4NLP - 2021</title>

  <!-- Bootstrap core CSS -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
  <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

  <!-- Custom styles for this template -->
  <link href="css/clean-blog.css" rel="stylesheet">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-NHJ3N6WW1Q"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-NHJ3N6WW1Q');
  </script>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="js/clean-blog.min.js"></script>
  <script>
    $(function(){
      $("#navbarResponsive").load("nav.html");
    });
  </script>
</head>

<body>

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
    <div class="container">
      <a class="navbar-brand" href="index.html">Eval4NLP</a>
      <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        Menu
        <i class="fas fa-bars"></i>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">

      </div>
    </div>
  </nav>

  <!-- Page Header -->
  <header class="masthead" style="background-image: url('img/home-bg.jpg')">
    <div class="overlay"></div>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <div class="site-heading">
            <h1>Eval4NLP 2021</h1>
            <span class="subheading">The 2<sup>nd</sup> Workshop on "Evaluation & Comparison of NLP Systems"</span>
            <span class="subheading"><b>10<sup>th</sup> November 2021</b>, co-located virtually at <a href="https://2021.emnlp.org/" target="blank">EMNLP 2021</a></span>
          </div>
        </div>
      </div>
    </div>
  </header>

  <!-- Main Content -->
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto">
        <h2 class="section-heading">Latest News</h2>
        <br>
        <table class="table">
          <tbody>
            <tr><th scope="row">Nov 10, 2021</th><td>Our workshop has been passed successfully. We would like to thank all authors, reviewers, steering committee, keynote speakers, sponsors, and participants for making this workshop fantastic. Hope to see you all again in the 3<sup>rd</sup> Eval4NLP workshop.</td></tr>
            <tr><th scope="row">Nov 10, 2021</th><td>The list of <a href="awards.html">best paper awards</a> has been announced. Congratulations!</td></tr>
            <tr><th scope="row">Nov 08, 2021</th><td>The list of keynote speakers and their talk abstracts has been added to our <a href="program.html#keynotes">Program</a>.</td></tr>
            <tr><th scope="row">Oct 28, 2021</th><td>The <a href="program.html">program</a> of the workshop has been published.</td></tr>
            <tr><th scope="row">Oct 10, 2021</th><td>The <a href="accepted_papers.html">list of accepted papers</a> has been published.</td></tr>
            <tr><th scope="row">Aug 31, 2021</th><td>As the CodaLab competition system is unstable, the organizers have decided to extend the submission deadline of the shared task to <b><span style="color:red">September 3, 2021</span></b>.</td></tr>
            <tr><th scope="row">Aug 20, 2021</th><td>The test phase of our shared task begins now! The test data is <a href="https://github.com/eval4nlp/SharedTask2021/tree/main/data/test21" target="blank">here</a>. Please don't forget to join <a href="https://groups.google.com/g/eval4nlp2021-shared-task" target="blank">our Google Group</a> for latest updates.</td></tr>
            <tr><th scope="row">Jul 25, 2021</th><td>The submission deadline of research papers has been extended to <b><span style="color:red">July 31, 2021</span></b>.</td></tr>
            <tr><th scope="row">Jul 20, 2021</th><td>The <a href="cfp.html#multiple_submission_policy">Multiple Submission Policy</a> and <a href="cfp.html#presenting_published_papers">Presenting Published Papers</a> sections in our call for papers have been updated.</td></tr>
            <tr><th scope="row">Jun 30, 2021</th><td><a href="https://competitions.codalab.org/competitions/33038" target="blank">The CodaLab competition</a> of our shared task is now live!</td></tr>
            <tr><th scope="row">Jun 16, 2021</th><td>Please join <a href="https://groups.google.com/g/eval4nlp2021-shared-task" target="blank">this Google Group</a> for posting questions related to our shared task.</td></tr>
            <tr><th scope="row">Jun 10, 2021</th><td><a href="https://github.com/eval4nlp/SharedTask2021#baseline" target="blank">The baseline</a> and <a href="https://github.com/eval4nlp/SharedTask2021/tree/main/annotation-guidelines" target="blank">the annotation guidelines</a> of our shared task are now available.</td></tr>
            <tr><th scope="row">May 24, 2021</th><td>We announce <a href="sharedtask.html">the shared task on "Explainable Quality Estimation"</a>.</td></tr>
            <tr><th scope="row">May 22, 2021</th><td><a href="https://www.softconf.com/emnlp2021/Eval4NLP/" target="blank">The submission system</a> is now open! <br>More details about preprints and supplementary materials are added to the <a href="cfp.html">Call for Papers</a>.</td></tr>
            <tr><th scope="row">May 14, 2021</th><td>We also welcome submissions from <a href="cfp.html#ARR">ACL Rolling Review</a>.</td></tr>
            <tr><th scope="row">Apr 22, 2021</th><td>The <a href="cfp.html">Call for Papers</a> is out!</td></tr>
            <tr><th scope="row">Apr 17, 2021</th><td>The <a href="https://aij.ijcai.org/" target="blank">Artificial Intelligence Journal (AIJ)</a> and <a href="https://www.salesforce.com/" target="blank">Salesforce</a> are our generous sponsors this year.</td></tr>
            <tr><th scope="row">Nov 19, 2020</th><td>Launch the workshop website</td></tr>
            <tr><th scope="row" style="width: 140px;"></th><td></td></tr>
          </tbody>
        </table>
        <h2 class="section-heading">Overview</h2>
        <p>Fair evaluations and comparisons are of fundamental importance to the NLP community to properly track progress, especially within the current deep learning revolution, with new state-of-the-art results reported in ever shorter intervals. This concerns the creation of benchmark datasets that cover typical use cases and blind spots of existing systems, the designing of metrics for evaluating the performance of NLP systems on different dimensions, and the reporting of evaluation results in an unbiased manner.</p>

        <p>Although certain aspects of NLP evaluation and comparison have been addressed in previous workshops (e.g., Metrics Tasks at WMT, NeuralGen, NLG-Evaluation, and New Frontiers in Summarization), we believe that new insights and methodology, particularly in the last 1-2 years, have led to much renewed interest in the workshop topic. The first workshop in the series, <a href="https://nlpevaluation2020.github.io/index.html" target="blank">Eval4NLP’20</a> (collocated with EMNLP’20), was the first workshop to take a broad and unifying perspective on the subject matter. We believe the second workshop will continue the tradition and become a reputed platform for presenting and discussing latest advances in NLP evaluation methods and resources.</p>

        <p>Particular topics of interest of the workshop include (but not limited to):</p>

        <ol>
          <li>
            <b>Designing evaluation metrics</b><br>
            Proposing and/or analyzing:
              <ul>
                <li>Metrics with <b>desirable properties</b>, e.g., high correlations with human judgments, strong in distinguishing high-quality outputs from mediocre and low-quality outputs, robust across lengths of input and output sequences, efficient to run, etc.;</li>
                <li><b>Reference-free</b> evaluation metrics, which only require source text(s) and system predictions; </li>
                <li><b>Cross-domain</b> metrics, which can reliably and robustly measure the quality of system outputs from heterogeneous modalities (e.g.,  image and speech), different genres (e.g., newspapers, Wikipedia articles and scientific papers) and different languages; </li>
                <li><b>Cost-effective</b> methods for eliciting high-quality manual annotations; and</li>
                <li>Methods and metrics for evaluating <b>interpretability and explanations</b> of NLP models</li>
              </ul>
          </li>

          <li>
            <b>Creating adequate evaluation data</b><br>
            Proposing new datasets or analyzing existing ones by studying their:
            <ul>
              <li><b>Coverage and diversity</b>, e.g., size of the corpus, covered phenomena, representativeness of samples, distribution of sample types, variability among data sources, eras, and genres; and</li>
              <li><b>Quality of annotations</b>, e.g., consistency of annotations, inter-rater agreement, and bias check</li>
            </ul>
          </li>

          <li>
            <b>Reporting correct results</b><br>
            Ensuring and reporting:
            <ul>
              <li><b>Statistics for the trustworthiness of results</b>, e.g., via appropriate significance tests, and reporting of score distributions rather than single-point estimates, to avoid chance findings;</li>
              <li><b>Reproducibility of experiments</b>, e.g., quantifying the reproducibility of papers and issuing reproducibility guidelines; and </li>
              <li><b>Comprehensive and unbiased error analyses</b> and case studies, avoiding cherry-picking and sampling bias.</li>
            </ul>
          </li>
        </ol>
        <p>See reference papers <a href="reference.html" class="underline">here</a>.</p>
        <br>
        <h3>Related Workshops</h3>
        <p><a href="https://humeval.github.io/">HumEval</a> invites submissions on all aspects of human evaluation of NLP systems.</p>
        <br>
        <h3>Contact us</h3>
        <p>Email: <a href="mailto:eval4nlp@gmail.com">eval4nlp@gmail.com</a></p>
        <br>
        <h3>Sponsors</h3>
        <div class="row sponsor-panel text-center">
        	<div class="col-lg-6 mx-auto">
          		<a href="https://aij.ijcai.org/" target="blank"><img src="img/aij_logo.jpg" class="sponsor-logo"></a>
          	</div>
          	<div class="col-lg-6 mx-auto">
          		<a href="https://www.salesforce.com/" target="blank"><img src="img/salesforce_logo.jpg" class="sponsor-logo"></a>
        	</div>
        </div>
      </div>
    </div>
  </div>

  <hr>

  <!-- Footer -->
  <footer>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <ul class="list-inline text-center">
            <li class="list-inline-item">
              <a href="https://twitter.com/nlp_evaluation">
                <span class="fa-stack fa-lg">
                  <i class="fas fa-circle fa-stack-2x"></i>
                  <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
            <li class="list-inline-item">
              <a href="mailto:eval4nlp@gmail.com">
                <span class="fa-stack fa-lg">
                  <i class="fas fa-circle fa-stack-2x"></i>
                  <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
          </ul>
          <p class="copyright text-muted">
            Copyright &copy; Eval4NLP 2021<br>
            Cover photo by <a href="https://www.pexels.com/photo/selective-focus-photo-of-person-writing-3807741/" target="blank">Andrea Piacquadio</a>
          </p>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
