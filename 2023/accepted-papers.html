<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Accepted Papers | Eval4NLP - 2023</title>

  <!-- Bootstrap core CSS -->
  <link href="../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="../vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">

  <!-- Custom styles for this template -->
  <link href="../css/clean-blog.css" rel="stylesheet">
  <link href="../css/custom.css" rel="stylesheet">

  <!-- Bootstrap core JavaScript -->
  <script src="../vendor/jquery/jquery.min.js"></script>
  <script src="../vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="../js/clean-blog.min.js"></script>
  <script>
    $(function(){
      $("#navbarResponsive").load("nav.html");
    });
  </script>
</head>

  <!-- Navigation -->
  <nav class="navbar  navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
    <div class="container">
      <a class="navbar-brand text-white" href="index.html">Eval4NLP 2023</a>
      <button class="navbar-toggler navbar-toggler-right text-white" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        Menu
        <i class="fas fa-bars"></i>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">

      </div>
    </div>
  </nav>

  <!-- Page Header -->
  <header class="masthead" style="background-image: url('img/home-bg.jpg')">
    <div class="overlay"></div>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <div class="site-heading">
            <h1>Accepted Papers</h1>
              <span class="subheading">The 4<sup>th</sup> Workshop on "Evaluation & Comparison of NLP Systems"</span>
              <span class="subheading"><b>1<sup>st</sup> November 2023</b>, co-located at <a href="www.ijcnlp-aacl2023.org/" target="blank">IJCNLP-AACL 2023</a></span>
          </div>
        </div>
      </div>
    </div>
  </header>

  <!-- Main Content -->
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto">
          <h1>Accepted Papers</h1>
          The following papers have been accepted to and will be presented at Eval4NLP this year.
          <p>
              <b>WRF: Weighted Rouge-F1 Metric for Entity Recognition</b>
              Lukas Jonathan Weber, Krishnan Jothi Ramalingam, Matthias Beyer and Axel Zimmermann
          </p>
        <p>
          <b>Assessing Distractors in Multiple-Choice Tests</b>
          Vatsal Raina, Adian Liusie and Mark Gales
        </p>
        <p>
          <b>Delving into Evaluation Metrics for Generation: A Thorough Assessment of How Metrics Generalize to Rephrasing Across Languages</b>
          Yixuan Wang, Qingyan Chen and Duygu Ataman
        </p>
        <p>
          <b>EduQuick: A Dataset Toward Evaluating Summarization of Informal Educational Content for Social Media</b>
          Zahra Kolagar, Sebastian Steindl and Alessandra Zarcone
        </p>
        <p>
          <b>Zero-shot Probing of Pretrained Language Models for Geography Knowledge</b>
          Nitin Ramrakhiyani, Vasudeva Varma, Girish Keshav Palshikar and Sachin Pawar
        </p>
        <p>
          <b>Transformers Go for the LOLs: Generating (Humourous) Titles from Scientific Abstracts End-to-End</b>
          Yanran Chen and Steffen Eger
        </p>
        <p>
          <b>Summary Cycles: Exploring the Impact of Prompt Engineering on Large Language Models’ Interaction with Interaction Log Information</b>
          Jeremy E Block, Yu-Peng Chen, Abhilash Budharapu, Lisa Anthony and Bonnie J Dorr
        </p>
        <p>
          <b>Large Language Models As Annotators: A Preliminary Evaluation For Annotating Low-Resource Language Content</b>
          Savita Bhat and Vasudeva Varma
        </p>
        <p>
          <b>Can a Prediction’s Rank Offer a More Accurate Quantification of Bias? A Case Study Measuring Sexism in Debiased Language Models</b>
          Jad Doughman, Shady Shehata, Leen Al Qadi, Youssef Nafea and Fakhri Karray
        </p>
        <h3>Shared Task</h3>
        <p>
          <b>The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics</b>
          Christoph Leiter, Juri Opitz, Daniel Deutsch, Yang Gao, Rotem Dror and Steffen Eger
        </p>
        <p>
          <b>HIT-MI&T Lab’s Submission to Eval4NLP 2023 Shared Task</b>
          Rui Zhang, Fuhai Song, Hui Huang, Jinghao Yuan, Muyun Yang and Tiejun Zhao
        </p>
        <p>
          <b>Understanding Large Language Model Based Metrics for Text Summarization</b>
          Abhishek Pradhan and Ketan Kumar Todi
        </p>
        <p>
          <b>LTRC_IIITH’s 2023 Submission for Prompting Large Language Models as Explainable Metrics Task</b>
          Pavan Baswani, Ananya Mukherjee and Manish Shrivastava
        </p>
        <p>
          <b>Which is better? Exploring Prompting Strategy For LLM-based Metrics</b>
          JoongHoon Kim, Sangmin Lee, Seung Hun Han, Saeran Park, Jiyoon Lee, Kiyoon Jeong and Pilsung Kang
        </p>
        <p>
          <b>Characterised LLMs Affect its Evaluation of Summary and Translation</b>
          Yuan Lu and Yu-Ting Lin
        </p>
        <p>
          <b>Reference-Free Summarization Evaluation with Large Language Models</b>
          Abbas Akkasi, Kathleen C. Fraser and Majid Komeili
        </p>
        <p>
          <b>Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task</b>
          Neema Kotonya, Saran Krishnasamy, Joel R. Tetreault and Alejandro Jaimes
        </p>
        <p>
          <b>Exploring Prompting Large Language Models as Explainable Metrics</b>
          Ghazaleh Mahmoudi
        </p>
        <p>
          <b>Team NLLG submission for Eval4NLP 2023 Shared Task: Retrieval-Augmented In-Context Learning for NLG Evaluation</b>
          Daniil Larionov, Vasiliy Viskov, George Kokush, Alexander Panchenko and Steffen Eger
        </p>
      </div>
    </div>
  </div>

  <hr>

  <!-- Footer -->
  <footer>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <ul class="list-inline text-center">
            <li class="list-inline-item">
              <a href="https://twitter.com/nlp_evaluation">
                <span class="fa-stack fa-lg">
                  <i class="fas fa-circle fa-stack-2x"></i>
                  <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
            <li class="list-inline-item">
              <a href="mailto:eval4nlp@gmail.com">
                <span class="fa-stack fa-lg">
                  <i class="fas fa-circle fa-stack-2x"></i>
                  <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
          </ul>
          <p class="copyright text-muted">
            Copyright &copy; Eval4NLP 2023<br>
          </p>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
