<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>References | Eval4NLP - 2023</title>

  <!-- Bootstrap core CSS -->
  <link href="../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="../vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
  <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

  <!-- Custom styles for this template -->
  <link href="../css/clean-blog.css" rel="stylesheet">
  <link href="../css/custom.css" rel="stylesheet">

  <!-- Bootstrap core JavaScript -->
  <script src="../vendor/jquery/jquery.min.js"></script>
  <script src="../vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="../js/clean-blog.min.js"></script>
  <script>
    $(function(){
      $("#navbarResponsive").load("nav.html");
    });
  </script>
</head>

<body>

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
    <div class="container">
      <a class="navbar-brand" href="index.html">Eval4NLP 2023</a>
      <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        Menu
        <i class="fas fa-bars"></i>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">

      </div>
    </div>
  </nav>

  <!-- Page Header -->
  <header class="masthead" style="background-image: url('img/home-bg.jpg')">
    <div class="overlay"></div>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <div class="site-heading">
            <h1>Eval4NLP 2023</h1>
            <span class="subheading">The 4<sup>th</sup> Workshop on "Evaluation & Comparison of NLP Systems"</span>
            <span class="subheading"><b>1<sup>st</sup> November 2023</b>, co-located at <a href="www.ijcnlp-aacl2023.org/" target="blank">IJCNLP-AACL 2023</a></span>
          </div>
        </div>
      </div>
    </div>
  </header>

  <!-- Main Content -->
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto">
        <ol>
          <li>Zhao et al. MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance. EMNLP 2019.</li>
          <li>Clark et al. Sentence Mover’s Similarity: Automatic Evaluation for Multi-Sentence Texts. ACL 2019.</li>
          <li>Louis and Nenkova. Automatically assessing machine summary content without a gold standard. Computational Linguistics, 39(2), 2013.</li>
          <li>Reimers and Gurevych. Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging. EMNLP 2017</li>
          <li>Dror et al. The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing. ACL 2018.</li>
          <li>Glavas et al. How to (Properly) Evaluate Cross-Lingual Word Embeddings: On Strong Baselines, Comparative Analyses, and Some Misconceptions. ACL 2019.</li>
          <li>Shen et al. Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms. ACL 2018.</li>
          <li>McCoy et al. Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference. ACL 2019.</li>
          <li>Dodge et al. Show Your Work: Improved Reporting of Experimental Results. EMNLP 2019</li>
          <li>Wu et al. Errudite: Scalable, Reproducible, and Testable Error Analysis. ACL 2019</li>
          <li>Böhm et al. Better Rewards Yield Better Summaries: Learning to Summarise Without References. EMNLP 2019</li>
          <li>Sun et al. How to Compare Summarizers without Target Length? Pitfalls, Solutions and Re-Examination of the Neural Summarization Literature. NeuralGen Workshop@NAACL 2019.</li>
          <li>Jin et al. Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment. arxiv e-print:1907.11932</li>
          <li>Peyrard. Studying Summarization Evaluation Metrics in the Appropriate Scoring Range. ACL 2019</li>
          <li>Peyrard and Eckle-Kohler. A Principled Framework for Evaluating Summarizers: Comparing Models of Summary Quality against Human Judgments. ACL 2017</li>
          <li>Owczarzak et al. An Assessment of the Accuracy of Automatic Evaluation in Summarization. In Workshop on Evaluation Metrics and System Comparison for Automatic Summarization. 2012</li>
          <li>Graham. Re-evaluating Automatic Summarization with BLEU and 192 Shades of ROUGE. EMNLP 2015</li>
          <li>Nenkova and Passonneau. Evaluating content selection in summarization: The pyramid method. NAACL 2004</li>
          <li>Zhao et al. XMoverScore: Evaluating Machine Translation without Human Reference. EurNLP 2019.</li>
          <li>Hase and Bansal. Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior? ACL 2020.</li>
          <li>Lertvittayakumjorn and Toni. Human-grounded Evaluations of Explanation Methods for Text Classification. EMNLP 2019.</li>
          <li>DeYoung et al. ERASER: A Benchmark to Evaluate Rationalized NLP Models. ACL 2020.</li>
          <li>Jacovi and Goldberg. Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness? ACL 2020.</li>
          <li>Gao et al. SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization. In: ACL 2020</li>
          <li>Zhao et al. On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation. In: ACL 2020</li>
          <li>Shikib and Eskenazi. USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation. In: ACL 2020</li>
          <li>Klebanov and Madnani. Automated Evaluation of Writing – 50 Years and Counting. In: ACL 2020</li>
          <li>Mueller et al. Cross-Linguistic Syntactic Evaluation of Word Prediction Models. In: ACL 2020</li>
          <li>Deutsch and Roth. SacreROUGE: An Open-Source Library for Using and Developing Summarization Evaluation Metrics. In: NLP-OSS 2020</li>
          <li>Deutsch et al. Towards Question-Answering as an Automatic Metric for Evaluating the Content Quality of a Summary. In: TACL 2021</li>
          <li>Deutsch et al. A Statistical Analysis of Summarization Evaluation Metrics Using Resampling Methods. In: TACL 2021</li>
          <li>Deutsch and Roth. Understanding the Extent to which Content Quality Metrics Measure the Information Quality of Summaries. In: CoNLL 2021</li>
          <li>Deutsch and Roth. Benchmarking Answer Verification Methods for Question Answering-Based Summarization Evaluation Metrics. In: ACL Findings 2022</li>
        </ol>
      </div>
    </div>
  </div>

  <hr>

  <!-- Footer -->
  <footer>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <ul class="list-inline text-center">
            <li class="list-inline-item">
              <a href="https://twitter.com/nlp_evaluation">
                <span class="fa-stack fa-lg">
                  <i class="fas fa-circle fa-stack-2x"></i>
                  <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
            <li class="list-inline-item">
              <a href="mailto:eval4nlp@gmail.com">
                <span class="fa-stack fa-lg">
                  <i class="fas fa-circle fa-stack-2x"></i>
                  <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
          </ul>
          <p class="copyright text-muted">
            Copyright &copy; Eval4NLP 2023<br>
            Cover photo by <a href="https://www.pexels.com/photo/person-holding-pen-pointing-at-graph-590020/" target="blank">Lukas</a>
          </p>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
