<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Shared Task | Eval4NLP - 2023</title>

  <!-- Bootstrap core CSS -->
  <link href="../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="../vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
  <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet'
    type='text/css'>
  <link
    href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800'
    rel='stylesheet' type='text/css'>

  <!-- Custom styles for this template -->
  <link href="../css/clean-blog.css" rel="stylesheet">
  <link href="../css/custom.css" rel="stylesheet">

  <!-- Bootstrap core JavaScript -->
  <script src="../vendor/jquery/jquery.min.js"></script>
  <script src="../vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="../js/clean-blog.min.js"></script>
  <script>
    $(function () {
      $("#navbarResponsive").load("nav.html");
    });
  </script>
</head>

<!-- Navigation -->
<nav class="navbar  navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand text-white" href="index.html">Eval4NLP 2023</a>
    <button class="navbar-toggler navbar-toggler-right text-white" type="button" data-toggle="collapse"
      data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
      aria-label="Toggle navigation">
      Menu
      <i class="fas fa-bars"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">

    </div>
  </div>
</nav>

<!-- Page Header -->
<header class="masthead" style="background-image: url('img/home-bg.jpg')">
  <div class="overlay"></div>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto">
        <div class="site-heading">
          <h1>Shared Task</h1>
          <span class="subheading">The 4<sup>th</sup> Workshop on "Evaluation & Comparison of NLP Systems"</span>
          <span class="subheading"><b>1<sup>st</sup> November 2023</b>, co-located at <a href="www.ijcnlp-aacl2023.org/"
              target="blank">IJCNLP-AACL 2023</a></span>
          <a class="btn btn-info" href="https://github.com/eval4nlp/SharedTask2023" role="button" target="blank" style="margin: 5px; width:150px;">GitHub</a>
          <a class="btn btn-info" href="https://codalab.lisn.upsaclay.fr/competitions/15072" role="button" target="blank" style="margin: 5px; width:150px;">CodaLab</a>
          <a class="btn btn-info" href="https://groups.google.com/g/eval4nlp2023-shared-task" role="button" target="blank" style="margin: 5px; width:150px;">Google Group</a>
        </div>
      </div>
    </div>
  </div>
</header>

<!-- Main Content -->
<div class="container">
  <div class="row">
    <div class="col-lg-8 col-md-10 mx-auto">
      <h2 class="section-heading mb-5">Prompting Large Language Models as Explainable Metrics</h2>
      <!-- <br>
          <h3>Latest News</h3>
          <br>
          <table class="table">
            <tbody>
              <tr><th scope="row">Month Day, Year</th><td>Content</span></b>.</td></tr>
              <tr><th scope="row" style="width: 140px;"></th><td></td></tr>
            </tbody>
          </table> -->


      <div class="mb-5">
        <h3>Important Dates</h3>
        <p>All deadlines are 11.59 pm UTC -12h (“Anywhere on Earth”). The timeframe of the test phase may change.
        <ul>
          <li><b>Share task announcement:</b> August 02, 2023</li>
          <li><b>Dev phase:</b> August 07, 2023</li>
          <li><b>Test phase:</b> September 18, 2023</li>
          <li><b>System Submission Deadline:</b> September 23, 2023</li>
          <li><b>System paper submission deadline:</b> October 5, 2023</li>
          <li><b>System paper camera ready submission deadline:</b> October 12, 2023</li>
        </ul>
        </p>
        <!-- <p class="alert alert-primary" role="alert">
            We will announce the important dates shortly. Please check back later.
          </p> -->
      </div>

      <div class="mb-5">
        <h3>Updates</h3>
        <ul>
          <li>August 15, 2023: Adding additional examples, resources and information regarding teams on this page.</li>
          <li>August 11, 2023: Adding information about submission tracks (further may follow)</li>
          <li>August 08, 2023: Release of the CodaLab Dev Competition (the test will be published there later)</li>
          <li>August 07, 2023: Release of Github Page with Baselines and extended model list</li>
        </ul>
      </div>

      <div class="mb-5">
        <h4>Overview</h4>
        <p>With groundbreaking innovations in unsupervised learning and scalable architectures the opportunities (but
          also
          risks) of automatically generating audio, images, video and text, seem overwhelming. Human evaluations of this
          content are costly
          and are often infeasible to collect. Thus, the need for automatic metrics that reliably judge the
          quality of generation systems and their outputs, is stronger than ever.
          Current state-of-the-art metrics for natural language generation (NLG) still do not match the performance of
          human experts. They are mostly based on black-box
          language models and usually return a single quality
          score (sentence-level), making it difficult to explain their internal decision process and their outputs.</p>

        <p>The release of APIs to large language models (LLMs), like ChatGPT and the recent open-source availability of
          LLMs like LLaMA has led to a boost of research in NLP, including LLM-based metrics.
          Metrics like GEMBA [7] explore the prompting of ChatGPT and GPT4 to directly leverage them as metrics.
          Instructscore [12]
          goes in a different direction and finetunes a LLaMA model to predict a fine grained error diagnosis
          of
          machine translated content. We notice that
          current work (1) does not systematically evaluate the vast amount of possible prompts and prompting
          techniques
          for metric usage, including, for example, approaches that explain a task to a model or let the model explain a
          task itself,
          and (2)
          rarely evaluates the performance of recent open-source LLMs, while their usage is incredibly important to
          improve
          the reproducibility of
          metric research, compared to closed-source metrics.</p>

        <p>This year’s Eval4NLP shared task, combines these two aspects. We provide
          a selection of open-source, pre-trained LLMs. The task is to develop strategies to extract
          scores from these LLM’s that grade machine translations and summaries. We will specifically focus on prompting
          techniques, therefore, fine-tuning of the LLM’s is not allowed.</p>

        <p>Based on the submissions, we hope to explore and formalize prompting approaches for open-source LLM-based
          metrics and, with
          that, help to improve their correlation to human judgements. As many prompting techniques produce explanations
          as a side product we hope that this task will also lead to more explainable metrics. Also, we want to evaluate
          which
          of the selected open-source models provide the best capabilities as metrics, thus, as a base for fine-tuning.
        </p>
      </div>

      <div class="mb-5">
        <h4>Goals</h4>
        The shared task has the following goals:
        <p><b>Prompting strategies for LLM-based metrics:</b> We want to explore which prompting strategies perform best
          for LLM-based metrics. E.g., few-shot prompting [1], where examples of other solutions are given in a
          prompt, chain-of-thought reasoning (CoT) [2], where the model is prompted to provide a multi-step
          explanation
          itself, or tree-of-thought prompting [3], where different explanation paths are considered, and the best
          is chosen. Also, automatic prompt generation might be considered [4]. Numerous other recent works explore
          further prompting strategies, some of which use multiple evaluation passes.
        </p>
        <p><b>Score aggregation for LLM-based metrics:</b> We also want to explore which strategies best aggregate the
          model scores from LLM-based metrics. E.g., scores might be extracted as the probability of a paraphrase being
          created [5, 6], or they could be extracted from LLM output directly [7].
        </p>
        <p><b>Explainability for LLM-based metrics:</b> We want to analyze whether the metrics that provide the best
          explanations (for example with CoT) will achieve the highest correlation to human judgements. We assume that
          this
          is the case, due to the human judgements being based on fine-grained evaluations themselves (e.g. MQM for
          machine translation)</p>
      </div>

      <div class="mb-5">
        <h3>Task Description</h3>
        <p>The task will consist of building a reference-free metric for machine translation and/or summarization that
          predicts sentence-level quality scores constructed from fine-grained scores or error labels. Reference-free
          means that the metric rates the provided machine translation solely based on the provided source
          sentence/paragraph, without any additional, human written references. Further,
          we note that many open-source LLMs have mostly been trained on
          English data, adding further challenges to the reference-free setup.</p>

        To summarize, the task will be structured as follows:
        <ul>
          <li>We provide a list of allowed LLMs from Huggingface</li>
          <li>Participants should use prompting to use these LLMs as metrics for MT and summarization</li>
          <li>Fine-tuning of the selected model(s) is <b>not allowed</b></li>
          <li>We will release baselines, which participants might build upon</li>
          <li>We will provide a CodaLab dashboard to compare participants' solutions to others</li>
        </ul>

        
        <p>
          As of August 7, we have released the first baselines in the shared task <a href="https://github.com/eval4nlp/SharedTask2023/tree/main"><b>github repository</b>b></a>.
        This repository also contains "train" and dev data. The CodaLab dashboard for the dev sets is available <a href="https://codalab.lisn.upsaclay.fr/competitions/15072">here</a>. 
        </p>

        <p>We will allow the following models from Huggingface:
        <ul>
          <li><a href="https://huggingface.co/TheBloke/guanaco-65B-GPTQ"><b>Guanaco-65B-GPTQ</b></a>: A four-bit quantized
            version of Guanaco-65B by [11]. We choose this model as its training data is multilingual to some extent.
          </li>
          <li><a href="https://huggingface.co/TheBloke/WizardLM-13B-V1.1-GPTQ"><b>WizardLM-13B-V1.1-GPTQ</b></a>: A four-bit quantized
            version of WizardLM-13B-V1.1 by [13]. We choose this model due to its good performance on leaderboards. We are
            not using the verions based on LLaMA2 as it might have been trained on recent Wikipedia articles, which we 
            will build our test sets on.
          </li>
          <li><a href="https://huggingface.co/NousResearch/Nous-Hermes-13b"><b>Nous-Hermes-13b</b></a>: A model by Nous Research. 
            We choose this model due to its good performance on leaderboards.
          </li>
        </ul>
        <p class="alert alert-primary" role="alert">We will update few further models throught the next days</p>
        </p>
        <img src="img/sharedtask.png" class="left-img">
        <p>This image shows an example of prompting based metrics. The sentences that should be evaluated are wrapped
          into a prompt that is passed to a language model as an input. Then the model generates an output that
          contains a score, a score is constructed from the output or a score is constructed from other model states.
        </p>
      </div>

      <div class="mb-5">
        <h4>Teams</h4>
        <p>Participants of the shared task might either work on it alone or in a team. When participants are working in a team, they should only use <b>one account</b> for submissions on the test task. 
        On the dev task, participants can submit with as many people and as often they like.</p>
      </div>

      <div class="mb-5">
        <h4>Tracks</h4>
        <p>We will offer two tracks based on the model sizes. One for models bigger than 25B parameters, one for smaller models.</p>
      </div>

      <div class="mb-5">
        <h4>Data</h4>
        <p>The <b>training and development data </b> for this shared task are the <a
            href="https://github.com/google-research/mt-metrics-eval" target="blank">MQM annotations of the WMT22
            metrics shared task</a> [8] for MT and the average of aspect based scores of <a
            href="https://github.com/Yale-LILY/SummEval" target="blank">SummEval</a> [9] for summarization. We note that
          the human
          annotations of SummEval are not based on phrase level. Thus, our test data might be quite different. Also, we
          will generally evaluate in reference-free settings, i.e. MT metrics will take a source sentence and a
          translation as input and summarization metrics will take a document and its summary as an input.</p>
      </div>

      <p>As <b>test data</b>, we collect a new dataset with sentence-level quality scores for MT and summarization.
        As source data, we leverage sentences and paragraphs collected from English Wikipedia pages created after
        01.01.2023. For MT, we follow the MQM annotation guidelines by [10]. We intend to provide the language
        pairs
        English-German, English-Chinese and English-Korean but might include others or “surprise” pairs. For
        summarization, we plan to apply an approach that
        uses MQM annotations to analyze the readability of the source text. Further, this approach annotates the
        factuality
        (presence of facts) by mapping facts between source and reference text. ditionally, we assess relevance by
        annotating each sentence in the source text with a relevance score. As for MT, we will construct paragraph level
        scores from this fine-grained annotation. To summarize, sentence-level scores will be constructed from fine
        grained scores, so metrics that are built in a similar way (return and aggregate fine-grained scores) might
        achieve better performance.

      <p class="alert alert-primary" role="alert">Examples of each annotation scheme will be released before the test
        phase takes place.</p>

      We reserve the right to change the evaluation languages and annotation scheme at later points.
      </p>


      <div class="mb-5">
        <h4>Evaluation</h4>
        <p>We follow the evaluation protocol of the WMT22 metrics shared task and evaluate with segment-level Kendall
          correlation.
          The shared task winner will be the systems that achieve the significantly highest correlation on the test
          set.
          It will be possible to only participate for MT or summarization.</p>

        <p>We also plan to perform a human evaluation of the explainability of each system submission as a separate
          track.
          More information on this will be available soon. </p>
      </div>

      <div class="mb-5">
        <h4>Baselines</h4>
        <p>
          As of August 7, we have released the first baselines in the shared task <a href="https://github.com/eval4nlp/SharedTask2023/tree/main">github repository</a>.
        This repository also contains "train" and dev data.
        </p>
      </div>

      <div class="mb-5">
        <h4>Examples</h4>
        <p>
          The following image shows exemplary prompting setups that might or might not prove useful in the shared task. Some of them will likely produce explanations of some sort
          while others will only return a score. A number of prompts have already been tested in recent works. E.g. [7] use a zero-shot setting, [6] use a translation probability setting
          and [12] aggregate sentence-level scores from a fine-grained error taxonomy (though they fine-tune their model).
        </p>
        <img src="img/examples.png" class="left-img">
      </div>

      <div class="mb-5">
        <h4>Submission</h4>
        <p class="alert alert-primary" role="alert">The CodaLab dashboard for the dev sets is available <a href="https://codalab.lisn.upsaclay.fr/competitions/15072">here</a>
          The data can be found in our <a href="https://github.com/eval4nlp/SharedTask2023/tree/main">github repository</a>.
          Submissions will require a single file with one score per line, matching the rows of the tsv files provided in our repository.
          For more details see the submission page.

          Final submissions will require further details, such as more detailed system descriptions, prompts and (if any) explanations. 
          The explanations are planned to be evaluated by humans.
        </p>
        <!-- <p>We use CodaLab as a platform for participants to submit their predictions for the test dataset. <b>The link to our CodaLab competition is <a href="TBD" target="blank">TBD</a>.</b></p> -->
      </div>

      <!-- <div class="mb-5">
        <h4>Submission Website</h4>
        TBD
      </div>-->


      <!-- The competition consists of two main phases.
        <ul>
          <li><strong>DEVELOPMENT PHASE</strong>: TBD
          </li>
          <li><strong>TEST PHASE</strong>: TBD
          </li>
        </ul>
        <br> 

        <h5 id="submission_format">Submission Format</h5>
        <br>
        TBD

        <h5>System Paper Submission</h5>
        <p>TBD
        </p>

        <h4>Awards for Best Submissions</h4>
        <p></p>
        -->


      <div class="mb-5">
        <h4>Recommended Resources</h4>
        <p>
          Resources related to prompting:
        <ul>
          <li><a href="https://www.promptingguide.ai/">promptingguide.ai/</a></li>
          <li><a href="https://github.com/promptslab/Awesome-Prompt-Engineering">Awesome-Prompt-Engineering</a></li>
          <li><a href="https://github.com/DukeLuo/awesome-awesome-prompts">awesome-awesome-prompts</a></li>
          <li><a href="https://github.com/snwfdhmp/awesome-gpt-prompt-engineering">awesome-gpt-prompt-engineering</a>
          </li>
          <li><a href="https://github.com/dqxiu/ICL_PaperList">ICL_PaperList</a></li>
          <li><a href="https://github.com/EgoAlpha/prompt-in-context-learning">prompt-in-context-learning</a></li>
        </ul>
          Amongst the many possible ways that can be used to structure and retrieve LLM output, the following tools might be considered:
        <ul>
          <li><a href="https://github.com/guidance-ai/guidance">Guidance</a></li>
          <li><a href="https://github.com/normal-computing/outlines">Outlines</a></li>
        </ul>
        </p>

        An exemplary selection of recent, related, prompt-based metrics:
        <ul>
          <li>Kocmi and Federmann, Large Language Models Are State-of-the-Art Evaluators of Translation Quality.
            arxiv-eprint: 2302.14520. 2023.</li>
          <li>Chiang and Lee, Can Large Language Models Be an Alternative to Human Evaluations?. In: ACL 2023.</li>
          <li>Xu et al., INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback.
            arxiv-eprint: 2305.14282. 2023.</li>
          <li>Liu et al., G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment. arxiv-eprint: 2303.16634.
            2023.</li>
          <li>Fu, et al.(2023). GPTScore: Evaluate as You Desire. arxiv-eprint: 2302.04166.</li>
        </ul>
      </div>

      <div class="mb-5">
        <h4>References</h4>
        [1] Brown et al., Language Models are Few-Shot Learners. In: NeurIPS </br>
        [2] Wei et al., Chain of Thought Prompting Elicits Reasoning in Large Language Models. In: Advances in Neural
        Information Processing Systems</br>
        [3] Yao et al., Tree of Thoughts: Deliberate Problem Solving with Large Language Models. ArXiv: 2305.10601</br>
        [4] Zhou et al., Large Language Models are Human-Level Prompt Engineers. In: ICLR 2023 Poster</br>
        [5] Thompson and Post, Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing.
        In: EMNLP2020</br>
        [6] Yuan et al., BARTScore: Evaluating Generated Text as Text Generation. In: Advances in Neural Information
        Processing Systems (Vol. 34, pp. 27263–27277)</br>
        [7] Kocmi and Federmann, Large Language Models Are State-of-the-Art Evaluators of Translation Quality. ArXiv:
        2302.14520 </br>
        [8] Freitag et al., Results of WMT22 Metrics Shared Task: Stop Using BLEU -- Neural Metrics Are Better and More
        Robust. In: WMT22</br>
        [9] Fabbri et al., SummEval: Re-evaluating Summarization Evaluation. In: Transactions of the Association for
        Computational Linguistics</br>
        [10] Freitag et al., Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine
        Translation. In: Transactions of the Association for Computational Linguistics</br>
        [11] Dettmers et al., QLoRA: Efficient Finetuning of Quantized LLMs. Arxiv: 2305.14314</br>
        [12] Xu et al., INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback.
        arxiv-eprint: 2305.14282. 2023.
        [13] Xu et al., WizardLM: Empowering Large Language Models to Follow Complex Instructions, Arxiv: 2304.12244
      </div>


      <div class="mb-5">
        <h4>Contact Information</h4>
        <ul>
          <li>Please join our <a href="https://groups.google.com/g/eval4nlp2023-shared-task">Google Group</a> for posting questions
            related to the shared task.</li>
          <li>If you want to contact the organizers privately, please send an email to <a
              href="mailto:eval4nlp@gmail.com" class="underline">eval4nlp@gmail.com</a>.</li>
        </ul>
      </div>

    </div>
  </div>
</div>

<hr>

<!-- Footer -->
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto">
        <ul class="list-inline text-center">
          <li class="list-inline-item">
            <a href="https://twitter.com/nlp_evaluation">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          <li class="list-inline-item">
            <a href="mailto:eval4nlp@gmail.com">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
        </ul>
        <p class="copyright text-muted">
          Copyright &copy; Eval4NLP 2023<br>
        </p>
      </div>
    </div>
  </div>
</footer>

</body>

</html>
